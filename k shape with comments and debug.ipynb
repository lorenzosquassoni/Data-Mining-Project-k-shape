{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-shape algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import multiprocessing\n",
    "from numpy.random import randint\n",
    "from numpy.linalg import norm, eigh\n",
    "from numpy.fft import fft, ifft\n",
    "from sklearn.base import ClusterMixin, BaseEstimator\n",
    "from sklearn.metrics import rand_score, normalized_mutual_info_score, adjusted_rand_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Z-score*** Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z-score is a mathematical approach to convert a \"Generic\" Random Variables to a Normal Random Variables. \n",
    "\n",
    "For more information about this approach: \n",
    "https://it.wikipedia.org/wiki/Standardizzazione_(statistica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Inputs**\n",
    "1. ```a```: numpy array containing time-series values;\n",
    "2. ```axis```: axis used to evaluate mean and standard deviation. For axis we intend rows or columns.\n",
    "3. ```ddof```: Degrees of freedom correction in the calculation of the standard deviation. Default value equals to 0\n",
    "\n",
    "**Function returned values**:\n",
    "1. Normalized Time-Series\n",
    "\n",
    "**Function Assumptions**:\n",
    "1. ```axis = 0 ```: normalize always on column axis. This is true beacuse we want to normalize time instants values, to exlude possible outliers or errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore(a, axis=0, ddof=0):\n",
    "    a = np.asanyarray(a) #convert the input a in an array\n",
    "    mns = a.mean(axis=axis) #compute the mean of the time series along the column axis (mean for each time instant)\n",
    "    sstd = a.std(axis=axis, ddof=ddof) #compute the standard deviation of the array along the column axis (mean for each time instant)\n",
    "\n",
    "    ## If Condition-->\n",
    "    ## If axis equals to row (axis = 1) and dimensions of mean array is lower than dimensions of time series array\n",
    "    ## -----> calculate Z score expanding dimensions of mean and std array on row axis\n",
    "    ## If axis equals to columns (axis = 0) or dimensions of mean array is equal to dimensions of time series array\n",
    "    ## -----> calculate Z score as time-series array minus its mean, divided by standard deviation\n",
    "\n",
    "    if axis and mns.ndim < a.ndim:\n",
    "        res = ((a - np.expand_dims(mns, axis=axis)) / #compute normalized data and expand the the mean and sd to match the dimension with the array a (the mean and std have 1 dimension less=\n",
    "               np.expand_dims(sstd, axis=axis))\n",
    "    else:\n",
    "        res = (a - mns) / sstd #compute normalized data\n",
    "\n",
    "    ## nan_to_num method --> replace NaN values with 0 or large integer values for infinite values\n",
    "    return np.nan_to_num(res)\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Roll_zeropad*** Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```roll_zeropad``` is a custom function to shift time series values to the right. The number of positions required by the movement are evaluated with parameter ```shift```.\n",
    "For each shift, a zero value is included in the time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Inputs**\n",
    "1. ```a```: numpy array containing time-series values;\n",
    "2. ```shift```: Number of right movements required;\n",
    "3. ```axis```: axis to consider for right movements.\n",
    "\n",
    "**Function returned values**:\n",
    "1. Shifted Time-Series\n",
    "\n",
    "**Function Assumptions**:\n",
    "1. ```axis = None ```: time series can be considered as a 1D array and not as a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll_zeropad(a, shift, axis=None):\n",
    "    # asanyarray method --> Convert the input to an ndarray, but pass ndarray subclasses through.\n",
    "    a = np.asanyarray(a) # Ensure that the input is ndarrary data type\n",
    "\n",
    "    # If no shift is required (shift = 0), it returns the original values of time series\n",
    "    if shift == 0:\n",
    "        return a\n",
    "\n",
    "    # If axis parameter is not evaluated (axis = None)\n",
    "    # ------> consider all elements of time series and not a specific dimensions, but this requires a reshape of data at the end\n",
    "    # If axis is set\n",
    "    # ------> extracted dimensions of a specific axis (rows or columns) and work on that\n",
    "    if axis is None:\n",
    "        n = a.size\n",
    "        reshape = True\n",
    "    else:\n",
    "        n = a.shape[axis]\n",
    "        reshape = False\n",
    "\n",
    "    # zeros_like() method --> Return an array of zeros with the same shape and type as a given array.\n",
    "\n",
    "    # If shift parameter is greater than size of the time series (but this requires that no axis are specified)\n",
    "    # ------> it evaluates an array of 0 values with the same size of a\n",
    "    if np.abs(shift) > n:\n",
    "        res = np.zeros_like(a)\n",
    "    # If shift parameter is lower than 0 (moving to the left)\n",
    "    # ------> subtract the shift value to n size of the array\n",
    "    # ------> extract the indices from 0 to (n-shift) values, considering a plain array (axis = None) or a specific dimensions\n",
    "    # ------> generate a 0-valued array with size [0, (n-shift)]\n",
    "    # ------> extract values from (n-shift) position to n position\n",
    "    # ------> create an array with values in range [(n-shift),n] at the beginning and all 0 values after\n",
    "    elif shift < 0:\n",
    "        shift += n\n",
    "        zeros = np.zeros_like(a.take(np.arange(n-shift), axis))\n",
    "        res = np.concatenate((a.take(np.arange(n-shift, n), axis), zeros), axis)\n",
    "    # If shift parameter is in the range [0, size of time series]\n",
    "    # ------> extract values from (n-shift) position to n position\n",
    "    # ------> generate a 0-valued array with size [(n-shift), n]\n",
    "    # ------> extract the indices from 0 to (n-shift) values, considering a plain array (axis = None) or a specific dimensions\n",
    "    # ------> create an array with 0 value at the beginning and then values in range [0, (n-shift)]\n",
    "    else:\n",
    "        zeros = np.zeros_like(a.take(np.arange(n-shift, n), axis))\n",
    "        res = np.concatenate((zeros, a.take(np.arange(n-shift), axis)), axis)\n",
    "\n",
    "    # If reshape is required\n",
    "    # -----> change shape (row, column) of shifted array, considering shape of time series values\n",
    "    if reshape:\n",
    "        return res.reshape(a.shape)\n",
    "    else:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***_ncc_c_3dim*** Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```_ncc_c_3dim``` is a custom function to evaluate the **array** of NCCc values. \\\n",
    "Each element of the array is associated with a specific shift computed on a specific time series.\\\n",
    "NCCc is a normalization of Cross Correlation value, which represents with a value the similarity between 2 time series\\\n",
    "K-Shape Algorithm uses NCCc to evaluate SBD distance measure which is used to assign the time series to the closest cluster centroid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Inputs**\n",
    "1. ```data```: numpy array containing 2 time series to compare;\n",
    "    * Generally, in the algorithm, X is the cluster centroid while Y is the time series to assign.\n",
    "\n",
    "**Function returned values**:\n",
    "1. The optimal value of NCCc between X and Y.\n",
    "\n",
    "**Function Assumptions**:\n",
    "1. ```None ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ncc_c_3dim(data):\n",
    "    # Extract X and Y from data\n",
    "    # Positions in data array are very important\n",
    "    x, y = data[0], data[1]\n",
    "\n",
    "    # np.norm function returns one of the eight matrix norms (X parameter considered is a 2D matrix.)\n",
    "    # Evaluating axis parameter specifies in which dimension evaluate matrix norm.\n",
    "    den = norm(x, axis=(0,1)) * norm(y, axis=(0,1))\n",
    "    #print(den)\n",
    "\n",
    "    # If the computed denominator of NCCc is so small, it is set to infinite value, in order to ignore NCCc value. \n",
    "    if den < 1e-9:\n",
    "        den = np.inf\n",
    "\n",
    "    # the variable represents the number of time instants considered for each time series.\n",
    "    x_len = x.shape[0]\n",
    "    #print(x_len)\n",
    "\n",
    "    # This is operation is necessary to improve computational effort of FFT.\n",
    "    # As the article said, in order to improve FFT performances, Cross Correlation must be an exact power-of-two.\n",
    "    # The following line of code approximate the result to the next power-of-two value. \n",
    "    fft_size = 1 << (2*x_len-1).bit_length()\n",
    "\n",
    "    # As the paper said, CC can be evaluated as convolution of two time series where one of two sequences is reversed in time domain. \n",
    "    # The convolution is computed as Inverse Discrete Fourier Transformer (IDFT) of the product of the individual Discrete Fourier Transforms (DFT) of the time series\n",
    "    # To reduce computational effort, Fast Fourier Transformer (FFT) substitutes DFT.\n",
    "    # np.conj function return the complex coniugate of a specified number. \n",
    "    # The complex conjugate of a complex number is obtained by changing the sign of its imaginary part.\n",
    "    cc = ifft(fft(x, fft_size, axis=0) * np.conj(fft(y, fft_size, axis=0)), axis=0)\n",
    "\n",
    "    # CC is the join, along axis 0 (rows), of two selected sequences\n",
    "    # The selected sequences are extracted from convolution of two time series (IFFT)\n",
    "    # CC is an array of complex numbers\n",
    "    cc = np.concatenate((cc[-(x_len-1):], cc[:x_len]), axis=0)\n",
    "    #print(cc)\n",
    "    \n",
    "    # Return array of NCCc values\n",
    "    return np.real(cc).sum(axis=-1) / den"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***_sbd*** Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```_sbd``` function is used to evaluate Shape Based Distance between two time series\\\n",
    "In particular, SBD is similarity measures applied to two time series.\\\n",
    "SBD values are in range [0,2] with 0 assigning perfect match\\\n",
    "```_sbd``` function does not return similarity value, but it returns the optimal shift y.\\\n",
    "The optimal shift of Y represents the closest shift of Y compared to X.\\\n",
    "\\\n",
    "This function is associated with **Algorithm 1** of the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Inputs**\n",
    "1. ```x```: reference time series\n",
    "    * Generally, in the algorithm, X is the cluster centroid.\n",
    "2. ```y```: compared time series\n",
    "    * Generally, in the algorithm, X is the cluster centroid.\n",
    "\n",
    "**Function returned values**:\n",
    "1. The optimal shift of Y.\n",
    "\n",
    "**Function Assumptions**:\n",
    "1. ```None ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is possible that the algorithm can ignore this function\n",
    "# In fact, if cluster centroids are evaluated as all zeros, the function is not used\n",
    "def _sbd(x, y):\n",
    "    # Evaluate NCCc array\n",
    "    ncc = _ncc_c_3dim([x, y])\n",
    "    #print(ncc)\n",
    "\n",
    "    # Find the index where NCCc is maximized (w in the paper)\n",
    "    idx = np.argmax(ncc)\n",
    "    #print(ncc[idx])\n",
    "\n",
    "    # The position w is used to evaluate the optimal shift as w-m\n",
    "    # The shift index is passed to roll_zeropad function to generate optimal y shift\n",
    "    yshift = roll_zeropad(y, (idx + 1) - max(len(x), len(y)))\n",
    "    \n",
    "    return yshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***collect_shift*** Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```collect_shift``` function is used compare a specific time series against a cluster centroid\\\n",
    "If the cluster centroid is represented by all zeros value, SBD distance is not evaluated\\\n",
    "\\\n",
    "It is part of the **Algorithm 2** presented in the paper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Inputs**\n",
    "1. ```data```: array of time series\n",
    "    * The first position of the array contains the time series to analyze (X).\n",
    "    * The second element is column vector, containing cluster centroids.\n",
    "\n",
    "**Function returned values**:\n",
    "1. Cluster centroid associated with specified time series.\n",
    "\n",
    "**Function Assumptions**:\n",
    "1. If cluster centroids are all zeros, single time series is considered as cluster centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_shift(data):\n",
    "    # Extract data from input parameter\n",
    "    x, cur_center = data[0], data[1]\n",
    "    #print(cur_center.shape)\n",
    "\n",
    "    # If all the elements of cluster centroids\n",
    "    # ------> Return the value of compared time series as cluster centroids\n",
    "    # Else\n",
    "    # ------> Return the optimal shift of X compared to current centroids.\n",
    "    if np.all(cur_center==0):\n",
    "        return x\n",
    "    else:\n",
    "        return _sbd(cur_center, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***_extract_shape*** Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```_extract_shape``` function returns optimal and normalized cluster centroids to consider in K-Shape Algorithm.\\\n",
    "\\\n",
    "Main steps considered are presented in **Algorithm 2** of the paper.\\\n",
    "As the article summarizes, cluster centroids are evaluated considering an optimization approach and a minimizer function.\\\n",
    "The main idea is to minimize the sum of squared distances among all other time series. \\\n",
    "In our case, as Cross Correlation captures the similarity, a maximizer function is considered in optimization problem.\\\n",
    "So, we have to maximize the squared similarities to all other time series sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Inputs**\n",
    "1. ```idx```: array containing cluster labels associated to each time series;\n",
    "2. ```x```: array of time series to analyze;\n",
    "3. ```j```: cluster to consider during the extraction;\n",
    "4. ```cur_center```: initial values of cluster centroid for cluster ***j***:\n",
    "    * Can be all zeros;\n",
    "    * A random selected time series\n",
    "    * Shape of ```cur_center``` is (len(<TIME_INSTANTS>, 1))\n",
    "\n",
    "**Function returned values**:\n",
    "1. Normalized Cluster Centroid for J Cluster\n",
    "\n",
    "**Function Assumptions**:\n",
    "1. As this approach is used in the context of iterative clustering, we use the previously computed centroid as reference and align all sequences towards this reference sequence.\n",
    "    * This is a reasonable choice because the previous centroid will be very close to the new centroid;\n",
    "    * SBD is used to evaluate optimal shift\n",
    "2. The maximizer function is associated to well-known maximization problem: maximization of the Rayleigh Quotient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_shape(idx, x, j, cur_center):\n",
    "    _a=[]\n",
    "    #print(idx)\n",
    "    # For each cluster label:\n",
    "    # -----> If it is equals to cluster to consider\n",
    "    # ---------> Find the centroid to consider (_sbd function used)\n",
    "    for i in range(len(idx)):\n",
    "        if idx[i] == j:\n",
    "            _a.append(collect_shift([x[i], cur_center]))\n",
    "\n",
    "    # Convert collected cluster centroids in a numpy array\n",
    "    a = np.array(_a)\n",
    "    #print(a)\n",
    "\n",
    "    # np.squeeze --> remove axes of length one from a\n",
    "    # If the previous code does not select anything, it means that noone of the time series is associated with J Cluster\n",
    "    # ------> Select a random index between 1 and number of time series (x.shape[0])\n",
    "    # ------> Return the time series associated with generated random index as cluster centroid\n",
    "    if len(a) == 0:\n",
    "        indices = np.random.choice(x.shape[0], 1)\n",
    "        return np.squeeze(x[indices].copy())\n",
    "        #return np.zeros((x.shape[1]))\n",
    "\n",
    "    # Extract column numbers of centroids --> time instants\n",
    "    columns = a.shape[1]\n",
    "\n",
    "    # Z-Normalization of centroids array\n",
    "    y = zscore(a, axis=1, ddof=1)\n",
    "\n",
    "    # Matrix preparation to apply Rayleigh Quotient maximization problem --> Algorithm 2 of paper\n",
    "    # np.dot --> dot products of two array\n",
    "    # np.empty --> create an empty array\n",
    "    # np.eye --> create identity matrix\n",
    "\n",
    "    # S matrix = product between centroids and theri transposed matrix\n",
    "    s = np.dot(y[:, :, 0].transpose(), y[:, :, 0])\n",
    "\n",
    "    # Create empty matrix P of dimensions [columns, columns]\n",
    "    p = np.empty((columns, columns))\n",
    "\n",
    "    # Fill the matrix p with a specific value, depending on problem dimensions\n",
    "    # With this operation, matrix P represents the product (1/m)*O evaluated in Algorithm 2\n",
    "    # matrix O is an all ones matrix\n",
    "    # m parameter is called columns in the code\n",
    "    p.fill(1.0/columns)\n",
    "\n",
    "    # Now matrix P represents matrix Q of the Algorithm 2\n",
    "    # It is the difference between the identity matrix and the product (1/m)*O\n",
    "    p = np.eye(columns) - p\n",
    "\n",
    "    # Evaluate matrix M with given formula\n",
    "    m = np.dot(np.dot(p, s), p)\n",
    "\n",
    "    # Calculate eigen vectors of matrix M\n",
    "    _, vec = eigh(m)\n",
    "\n",
    "    # Extract first centroid of matrix M --> it represents the centroid for specific cluster J\n",
    "    # These equations depends on Rayleigh Quotient maximization problem\n",
    "    centroid = vec[:, -1]\n",
    "\n",
    "    print((centroid.reshape((x.shape[1], 1)).shape))\n",
    "    # print(a - centroid.reshape((x.shape[1], 1)))\n",
    "    # print((a - centroid.reshape((x.shape[1], 1))).shape)\n",
    "    # print(np.linalg.norm(a - centroid.reshape((x.shape[1], 1)), axis=(1, 2)))\n",
    "\n",
    "    # Vector norm --> non negative real number which could represent the distance from the origin in a vector space\n",
    "\n",
    "    # The following operations create a matrix where each time series is differenced with centroid values\n",
    "    # Then, the equations try to evaluate the distance from the origin of an ideal vector space\n",
    "    # This approach is repeated adding centroid values with time series values\n",
    "    finddistance1 = np.sum(np.linalg.norm(a - centroid.reshape((x.shape[1], 1)), axis=(1, 2)))\n",
    "    finddistance2 = np.sum(np.linalg.norm(a + centroid.reshape((x.shape[1], 1)), axis=(1, 2)))\n",
    "\n",
    "    # If the first distance is greater, you need to consider the symmetric quadrant\n",
    "    # So, centroid values are multiplied with -1.\n",
    "    if finddistance1 >= finddistance2:\n",
    "        centroid *= -1\n",
    "\n",
    "    # Return normalized centroid for cluster J\n",
    "    return zscore(centroid, ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***_kshape*** Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```_kshape``` function represents the real intelligence behind K-Shape algorithm\\\n",
    "\\\n",
    "Main steps considered are presented in **Algorithm 3** of the paper.\\\n",
    "The general idea is to apply kshape approach until a stopping condition is reached.\\\n",
    "There are two possible stopping conditions:\n",
    "- Maximum number of iterations is reached;\n",
    "- In two consecutive iterations, the same output is produced.\\\n",
    "\n",
    "\n",
    "\\\n",
    "K-Shape Algorithm is composed by two main steps:\n",
    "* Refinement step: during this step, new cluster centroids are evaluated;\n",
    "* Assigment step: it describe the assigment of a specific time series to the closest cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Inputs**\n",
    "1. ```x```: array of time series to analyze; \n",
    "2. ```k```: number of cluster to find out;\n",
    "3. ```centroid_init```: how centroids values are initialized:\n",
    "    * Possible values are ```zero``` or ```random```\n",
    "    * Default value is ```zero```\n",
    "4. ```max_iter```: maximum number of iterations executed by the algorithm:\n",
    "    * Default values is 100.\n",
    "5. ```n_jobs```: this parameter activates the multiprocessing approach inside Python. \n",
    "    * This could be useful to improve algorithm computational time;\n",
    "    * Since multiprocessing is quite heavy, default value of parameter ```n_jobs``` is 1\n",
    "\n",
    "**Function returned values**:\n",
    "1. Assigned sequence: an array where the value of ith element represents the cluster where ith time series joins.\n",
    "2. Extracted centroids: array of extracted centroids.\n",
    "\n",
    "**Function Assumptions**:\n",
    "1. Except from the first iteration, previous evaluated centroids are used to extract new centroids values. \n",
    "2. The first assignment sequence is randomly generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _kshape(x, k, centroid_init='zero', max_iter=100, n_jobs=1):\n",
    "    # Extract number of time series to analyze\n",
    "    m = x.shape[0]\n",
    "\n",
    "    # Generate a random sequence assigment --> necessary only in the first execution\n",
    "    idx = randint(0, k, size=m)\n",
    "\n",
    "    # Initial centroid values\n",
    "    # Zero value generated all centroids as zeros array\n",
    "    # Random value extract k time series from available ones and use them as cluster centroids.\n",
    "    if centroid_init == 'zero':\n",
    "        centroids = np.zeros((k, x.shape[1], x.shape[2]))\n",
    "    elif centroid_init == 'random':\n",
    "        indices = np.random.choice(x.shape[0], k)\n",
    "        centroids = x[indices].copy()\n",
    "    \n",
    "    # Distances matrix is used to track distances of each time series from all clusters\n",
    "    # It is initialized as empty\n",
    "    distances = np.empty((m, k))\n",
    "    \n",
    "    # For istruction is necessary to apply the first stopping condition --> the maximum number of iterations is reached\n",
    "    for it in range(max_iter):\n",
    "        # This assignment is necessary for the second stopping condition\n",
    "        # With old_idx, we can compare if the previously found sequence is equal to the current one.\n",
    "        old_idx = idx\n",
    "\n",
    "        # Refinement step\n",
    "        # Using _extract_shape function, new cluster centroids are evaluated on each iteration\n",
    "        # To extract new cluster centroids, previous ones are used\n",
    "        # This is applied for each cluster\n",
    "\n",
    "        for j in range(k):\n",
    "            for d in range(x.shape[2]):\n",
    "                centroids[j, :, d] = _extract_shape(idx, np.expand_dims(x[:, :, d], axis=2), j, np.expand_dims(centroids[j, :, d], axis=1))\n",
    "                #centroids[j] = np.expand_dims(_extract_shape(idx, x, j, centroids[j]), axis=1)\n",
    "\n",
    "        # Using multiprocessing to apply multi threading of the same tasks\n",
    "        pool = multiprocessing.Pool(n_jobs)\n",
    "        args = []\n",
    "        for p in range(m):\n",
    "            for q in range(k):\n",
    "                args.append([x[p, :], centroids[q, :]])\n",
    "        # The task is NCC-C evaluation for each time series to all cluster centroids\n",
    "        # Arguments of the task are time series and cluster centroids\n",
    "        # A specific comparison represents a single task\n",
    "\n",
    "        # Extract results\n",
    "        result = pool.map(_ncc_c_3dim, args)\n",
    "\n",
    "        # Close pool\n",
    "        pool.close()\n",
    "\n",
    "        # R index is necessary to access results of multiprocessing\n",
    "        r = 0\n",
    "\n",
    "        # For each time series p in m\n",
    "        for p in range(m):\n",
    "            # For each cluster q in k\n",
    "            for q in range(k):\n",
    "                # Distance matrix in [p,q] contains SBD value\n",
    "                # SBD value represents the similarity between time series p and cluster centroids q \n",
    "                distances[p, q] = 1 - result[r].max()\n",
    "                r = r + 1\n",
    "\n",
    "        # Using argmin function, for each row of distance matrix (so, for each time series) the column index with the lowest value is extracted\n",
    "        # So, this istruction extracts for each time series the column (so, the cluster) with the lowest SBD value\n",
    "        idx = distances.argmin(1)\n",
    "\n",
    "        # Second stopping conditions --> two consecutive sequences are equal\n",
    "        if np.array_equal(old_idx, idx):\n",
    "            break\n",
    "    \n",
    "    # Return extracted sequence and cluster centroids values\n",
    "    return idx, centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***kshape*** Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kshape(x, k, centroid_init='zero', max_iter=100):\n",
    "    idx, centroids = _kshape(np.array(x), k, centroid_init=centroid_init, max_iter=max_iter)\n",
    "    clusters = []\n",
    "    for i, centroid in enumerate(centroids):\n",
    "        series = []\n",
    "        for j, val in enumerate(idx):\n",
    "            if i == val:\n",
    "                series.append(j)\n",
    "        clusters.append((centroid, series))\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***KShapeClusteringCPU*** Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KShapeClusteringCPU(ClusterMixin,BaseEstimator):\n",
    "    labels_= None\n",
    "    centroids_ = None\n",
    "\n",
    "    def __init__(self,n_clusters, centroid_init='zero', max_iter=100, n_jobs=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.centroid_init = centroid_init\n",
    "        self.max_iter = max_iter\n",
    "        if n_jobs is None:\n",
    "            self.n_jobs=1\n",
    "        elif n_jobs == -1:\n",
    "            self.n_jobs = multiprocessing.cpu_count()\n",
    "        else:\n",
    "            self.n_jobs=n_jobs\n",
    "        \n",
    "\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        clusters = self._fit(X,self.n_clusters, self.centroid_init, self.max_iter,self.n_jobs)\n",
    "        self.labels_ = np.zeros(X.shape[0])\n",
    "        self.centroids_ =np.zeros((self.n_clusters, X.shape[1], X.shape[2]))\n",
    "        for i in range(self.n_clusters):\n",
    "            self.labels_[clusters[i][1]] = i\n",
    "            self.centroids_[i]=clusters[i][0]\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        labels, _ = self._predict(X,self.centroids_)\n",
    "        return labels\n",
    "        \n",
    "    \n",
    "    def _predict(self,x, centroids):\n",
    "        m = x.shape[0]\n",
    "        idx = randint(0, self.n_clusters, size=m)\n",
    "        distances = np.empty((m, self.n_clusters))\n",
    "        \n",
    "\n",
    "    \n",
    "        pool = multiprocessing.Pool(self.n_jobs)\n",
    "        args = []\n",
    "        for p in range(m):\n",
    "            for q in range(self.n_clusters):\n",
    "                args.append([x[p, :], centroids[q, :]])\n",
    "        result = pool.map(_ncc_c_3dim, args)\n",
    "        pool.close()\n",
    "        r = 0\n",
    "        for p in range(m):\n",
    "            for q in range(self.n_clusters):\n",
    "                distances[p, q] = 1 - result[r].max()\n",
    "                r = r + 1\n",
    "    \n",
    "        idx = distances.argmin(1)\n",
    "\n",
    "        return idx, centroids\n",
    "    \n",
    "    \n",
    "    def _fit(self,x, k, centroid_init='zero', max_iter=100,n_jobs=1):\n",
    "        idx, centroids = _kshape(np.array(x), k, centroid_init=centroid_init, max_iter=max_iter, n_jobs=n_jobs)\n",
    "        clusters = []\n",
    "        for i, centroid in enumerate(centroids):\n",
    "            series = []\n",
    "            for j, val in enumerate(idx):\n",
    "                if i == val:\n",
    "                    series.append(j)\n",
    "            clusters.append((centroid, series))\n",
    "    \n",
    "        return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlated Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```ClusterDataLoader``` class is used to read dataset input files and convert them in a set of arrays. \\\n",
    "The function will load every files (both TRAIN and TEST files) in a specific path. \\\n",
    "Every files must be a CSV files with comma separator\\\n",
    "\\\n",
    "Each row of the file represents a time-series and it contains:\n",
    "* The first element is cluster which time-series joins;\n",
    "* All successive elements are measurements of each time-series. Each measurements is associated with a specific time instants. \n",
    "\n",
    "It is important to highlight that there aren't any information about time-series or time instants order. \\\n",
    "We suppose that the order of time-series is not important for K-shape Clustering Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterDataLoader:\n",
    "    def __init__(self, dataset_path):\n",
    "        self.path = dataset_path\n",
    "\n",
    "    def load(self, sub_dataset_name):\n",
    "        ts, labels = [], []\n",
    "        for mode in ['_TRAIN']:\n",
    "            with open(os.path.join(self.path, sub_dataset_name, sub_dataset_name + mode)) as csv_file:\n",
    "                lines = csv.reader(csv_file, delimiter=',')\n",
    "                for line in lines:\n",
    "                    ts.append([float(x) for x in line[1:]])\n",
    "                    labels.append(int(line[0])-1)\n",
    "\n",
    "        if min(labels) == 1:\n",
    "            labels = labels - 1\n",
    "        if min(labels) == -1:\n",
    "            labels = labels + 1\n",
    "\n",
    "        return np.array(ts), np.array(labels), int(len(set(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Shape Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a subset (100 hundred rows) of ```CROP``` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'dataset/univariate_example/'\n",
    "DATASET_NAME = 'Crop'\n",
    "\n",
    "dataloder = ClusterDataLoader(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts, labels, num_clusters = dataloder.load(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Timeseries Dataset\n",
      "-------------------------\n",
      "\n",
      "[[-1.04911634 -0.96685745 -0.88459857 ... -0.51685297 -0.59911186\n",
      "  -0.68137074]\n",
      " [-1.10145498 -1.1413895  -1.18132403 ... -1.02158593 -1.10544843\n",
      "  -1.18931093]\n",
      " [-0.80873226 -0.98601697 -1.16330168 ... -0.78711217 -0.70063182\n",
      "  -0.61415147]\n",
      " ...\n",
      " [-1.62102196 -1.41436412 -1.20770628 ... -1.28672251 -0.75184341\n",
      "  -0.2169643 ]\n",
      " [-1.0246006  -1.48431623 -1.36938732 ... -1.04614977 -0.39967465\n",
      "   0.24680046]\n",
      " [-1.10748362 -1.14969269 -1.19190177 ... -0.67695102 -0.19154663\n",
      "   0.29385776]]\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------------\\nTimeseries Dataset\\n-------------------------\\n\")\n",
    "print(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Timeseries Dataset Dimensions\n",
      "-------------------------\n",
      "\n",
      "(100, 46)\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------------\\nTimeseries Dataset Dimensions\\n-------------------------\\n\")\n",
    "print(ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Clusters Dataset\n",
      "-------------------------\n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------------\\nClusters Dataset\\n-------------------------\\n\")\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Clusters Dataset Dimensions\n",
      "-------------------------\n",
      "\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------------------\\nClusters Dataset Dimensions\\n-------------------------\\n\")\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(num_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Shape Single Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 1)\n",
      " Execution time: 0.0734097957611084\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "ksc = KShapeClusteringCPU(n_clusters=num_clusters,max_iter=2,n_jobs=-1)\n",
    "ksc.fit(np.expand_dims(ts, axis=2))\n",
    "print(f' Execution time: {time.time() - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monovariate Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monovariate time-series can be represented as a matrix NxM with:\n",
    "- N (rows) as the number of time-series. Each row represent a time series which can be connected to a specific date \n",
    "- M (columns) as time instants considered for each time series. Each time instants respects a frequency of analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of monovariate array of time series: 4 time series and 3 time instants\n",
    "##\n",
    "a_monovariate = np.array([[10, 12, 31],\n",
    "              [4, 9, 10],\n",
    "              [70, 81, 2],\n",
    "              [12, 112, 12]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate time-series can be represented as a matrix NxM with:\n",
    "- N (rows) as the number of time-series. Each row represent a time series which can be connected to a specific date \n",
    "- M (columns) as time instants considered for each time series. Each time instants respects a frequency of analysis \n",
    "\n",
    "The main difference with monovariate time-series is that each cell of the matrix is an array of values. The number of elements of the array can be considered as the number of variables involved, in addition to timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean\n",
      "-----------\n",
      "[[ 24.33333333 353.06666667]\n",
      " [ 12.33333333  14.70366667]\n",
      " [153.33333333  65.66666667]]\n",
      "Std\n",
      "-----------\n",
      "[[ 14.81740718 458.080352  ]\n",
      " [  9.84321537  12.21227362]\n",
      " [139.30382463  19.43078886]]\n"
     ]
    }
   ],
   "source": [
    "#Example of multivariate array of time series:\n",
    "\n",
    "#2 time series, with 3 time instants and 2 variables\n",
    "\n",
    "a_multivariate = np.array([[[11, 1000], [25, 30], [350, 40]],\n",
    "                           [[45, 0.2], [1, 0.111], [65, 70]],\n",
    "                           [[17, 59], [11, 14], [45, 87]]])\n",
    "\n",
    "#compute the mean for each time instant\n",
    "print(\"Mean\\n-----------\")\n",
    "print(a_multivariate.mean(axis=0))\n",
    "print(\"Std\\n-----------\")\n",
    "print(a_multivariate.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output can be explained as:\n",
    "- Cell 0,0 ==> mean (or std) of first elements of first time instant;\n",
    "- Cell 0,1 ==> mean (or std) of second elements of first time instant;\n",
    "- Cell 1,0 ==> mean (or std) of first elements of second time instant;\n",
    "....\n",
    "\n",
    "So the output is a matrix Nx(M-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Time Series - zScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.89984254  1.41227042]\n",
      "  [ 1.28684238  1.25253772]\n",
      "  [ 1.41178225 -1.32092767]]\n",
      "\n",
      " [[ 1.39475594 -0.77031609]\n",
      "  [-1.15138528 -1.19491809]\n",
      "  [-0.63410559  0.22301376]]\n",
      "\n",
      " [[-0.4949134  -0.64195433]\n",
      "  [-0.13545709 -0.05761963]\n",
      "  [-0.77767666  1.09791391]]]\n"
     ]
    }
   ],
   "source": [
    "#we apply the zscore function\n",
    "zscore_result = zscore(a_multivariate)\n",
    "print(zscore_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monovariate Time Series - zScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.52393683 -0.93494794  1.62139887]\n",
      " [-0.74848119 -1.00253454 -0.35247801]\n",
      " [ 1.72150673  0.61954382 -1.10443111]\n",
      " [-0.44908871  1.31793866 -0.16448974]]\n"
     ]
    }
   ],
   "source": [
    "#we apply the zscore function\n",
    "zscore_result = zscore(a_monovariate, axis=0)\n",
    "print(zscore_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monovariate Time Series - Shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10  12  31]\n",
      " [  4   9  10]\n",
      " [ 70  81   2]\n",
      " [ 12 112  12]]\n"
     ]
    }
   ],
   "source": [
    "print(a_monovariate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0  10  12]\n",
      " [ 31   4   9]\n",
      " [ 10  70  81]\n",
      " [  2  12 112]]\n"
     ]
    }
   ],
   "source": [
    "a_monovariate_shifted = roll_zeropad(a_monovariate, 1)\n",
    "print(a_monovariate_shifted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating parameter ```shift``` to 1, the function shifts time series to the right of one position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12  31   4]\n",
      " [  9  10  70]\n",
      " [ 81   2  12]\n",
      " [112  12   0]]\n"
     ]
    }
   ],
   "source": [
    "a_monovariate_shifted = roll_zeropad(a_monovariate, -1)\n",
    "print(a_monovariate_shifted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating parameter ```shift``` to -1, the function shifts time series to the left of one position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0]\n",
      " [ 0  0 10]\n",
      " [12 31  4]\n",
      " [ 9 10 70]]\n"
     ]
    }
   ],
   "source": [
    "a_monovariate_shifted = roll_zeropad(a_monovariate, 5)\n",
    "print(a_monovariate_shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4   9  10]\n",
      " [ 70  81   2]\n",
      " [ 12 112  12]\n",
      " [  0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "a_monovariate_shifted = roll_zeropad(a_monovariate, -3)\n",
    "print(a_monovariate_shifted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Time Series - Shifting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.10e+01 1.00e+03]\n",
      "  [2.50e+01 3.00e+01]\n",
      "  [3.50e+02 4.00e+01]]\n",
      "\n",
      " [[4.50e+01 2.00e-01]\n",
      "  [1.00e+00 1.11e-01]\n",
      "  [6.50e+01 7.00e+01]]\n",
      "\n",
      " [[1.70e+01 5.90e+01]\n",
      "  [1.10e+01 1.40e+01]\n",
      "  [4.50e+01 8.70e+01]]]\n"
     ]
    }
   ],
   "source": [
    "print(a_multivariate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.00e+00 1.10e+01]\n",
      "  [1.00e+03 2.50e+01]\n",
      "  [3.00e+01 3.50e+02]]\n",
      "\n",
      " [[4.00e+01 4.50e+01]\n",
      "  [2.00e-01 1.00e+00]\n",
      "  [1.11e-01 6.50e+01]]\n",
      "\n",
      " [[7.00e+01 1.70e+01]\n",
      "  [5.90e+01 1.10e+01]\n",
      "  [1.40e+01 4.50e+01]]]\n"
     ]
    }
   ],
   "source": [
    "a_multivariate_shifted = roll_zeropad(a_multivariate, 1)\n",
    "print(a_multivariate_shifted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shifting works across the same row. Considering that each element of row has an array of values, which indexes means different variables to analyze (temp, humidity, etc), this can generate misunderstanding data because the operation changes the variable associated to data value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monovariate Time Series - NCCc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[2,5]])\n",
    "y = np.array([[2,5]])\n",
    "\n",
    "nccc = _ncc_c_3dim([x,y])\n",
    "\n",
    "print(nccc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBD: [[2 5]]\n"
     ]
    }
   ],
   "source": [
    "sbd = _sbd(x,y)\n",
    "\n",
    "print(f\"SBD: {sbd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599\n"
     ]
    }
   ],
   "source": [
    "x_len = 300\n",
    "xlen_trans = 2*x_len-1\n",
    "print(xlen_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(xlen_trans.bit_length())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "print (1 << xlen_trans.bit_length())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    }
   ],
   "source": [
    "print(2 ** 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7.+0.j -3.+0.j]\n"
     ]
    }
   ],
   "source": [
    "print(fft([2,5],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(np.zeros((2, 2, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read HDF File Format - Multivariate Time Series Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['data']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"data\": shape (409, 405, 61), type \"<f8\">\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('dataset/multivariate_example/Heartbeat/Heartbeat_DATA.h5', 'r') as f: \n",
    "    print(f.keys())\n",
    "    dset = list(f['data'])\n",
    "    print(f['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(405, 61)\n"
     ]
    }
   ],
   "source": [
    "first_element = dset[0]\n",
    "print(first_element.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(405, 61)\n",
      "[[ 0.67344416  1.03344607  0.51781406 ...  2.60632811  1.78939962\n",
      "  -0.92018153]\n",
      " [ 0.27982926  1.97446527  2.52269576 ...  0.9516411  -0.4143421\n",
      "   3.00223469]\n",
      " [-1.05258329  1.70247735  1.72015895 ...  1.19972818  1.77575933\n",
      "   4.79457689]\n",
      " ...\n",
      " [-1.13371095 -0.16836813 -1.03306481 ... -0.75880258 -0.7104573\n",
      "   0.53581495]\n",
      " [-0.77082255 -0.90313236 -0.98622583 ... -0.20830294  0.13230556\n",
      "  -0.54074809]\n",
      " [-0.83482596 -1.52393233 -1.22096163 ...  1.03060257  1.05737061\n",
      "   0.4729672 ]]\n"
     ]
    }
   ],
   "source": [
    "second_element = dset[1]\n",
    "print(second_element.shape)\n",
    "print(second_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First shape element is time series number\n",
      "Secondo shape element is the number of variables considered for each time series\n",
      "Third element is the number of time instants considered for each measurement\n"
     ]
    }
   ],
   "source": [
    "print(\"First shape element is time series number\")\n",
    "print(\"Secondo shape element is the number of variables considered for each time series\")\n",
    "print(\"Third element is the number of time instants considered for each measurement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
